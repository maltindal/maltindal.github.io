---
layout: post
title:  "Morse telegraph using Web Audio API"
date:   2016-05-1 07:12:55 +0200
categories: web
custom_css:
- morse
math: true
---
## Introduction

"Morse code is a method of transmitting text information as a series of
on-off tones, lights, or clicks that can be directly understood by a
skilled listener or observer without special equipment." [[wikipedia][Morse_code]]
The morse code of a letter or number is a sequence of short and long signals.

```
{
  "•–":   "A", "–•••": "B", "–•–•": "C", "–••":  "D",
  "•":    "E", "••–•": "F", "––•":  "G", "••••": "H",
  "••":   "I", "•–––": "J", "–•–":  "K", "•–••": "L",
  "––":   "M", "–•":   "N", "–––":  "O", "•––•": "P",
  "––•–": "Q", "•–•":  "R", "•••":  "S", "–":    "T",
  "••–":  "U", "•••–": "V", "•––":  "W", "–••–": "X",
  "–•––": "Y", "––••": "Z"
}
```
This is a map of morse code to letter.

{% include morse.html %}

The telegraph above is an [implementation][impl] for coding letters using the morse code.
Use the "m" key for typing morse codes. Hold the "m" key longer the produce a long signal,
and press the key "m" briefly in order to produce a short signal.

## Web Audio API

The specification of Web Audio API contains the details to the JavaScript API
for processing audio in web applications as well as a structured and helpful
introductory to the architecture its ideas.
From an abstract perspective the [architecture][WebAudioAPI_architecture]
of the Web Audio API is similar to what is known as "pipes-and-filters".
A filter in this case is called AudioNode and multiple AudioNodes can be
connected by setting their source and destination attributes accordingly.
The AudioNodes are being connected within an AudioContext which has a single
AudioDestinationNode where the destination node is routed to an output device,
for instance the speakers.

## Producing the sound

Instead of playing a tone that is existing on the server, for instance as a
wave file, the implementation creates the sound data on the fly.
For this purpose the ScriptProcessorNode can be used. It is an AudioNode for
generation, processing or analysis of audio using JavaScript.
This node type is deprecated and is going to be replaced by the AudioWorkerNode.
When pressing the key "m" a signal at 440 Hz frequency is generated.
The ScriptProcessorNode instance has an `onaudioprocess` listener that is
used to process audio while the associated nodes are connected.
The listener can optionally read the input audio data from the passed
AudioProcessingEvent object, then do the processing based on the optional
input data and finally assign the result audio data to the `outputBuffer` of
the event.

In case of the morse implementation, no input data needs to be read.
Instead the data is generated that corresponds to a tone at 440 Hz, which
corresponds to the [musical note A][A440_pitch_standard] above middle C.
There is no real reason why I took 440 Hz. The ScriptProcessorNode instance
is created by calling the function [createScriptProcessor][WebAudioAPI_createScriptProcessor].
The first argument is the buffer size, where the size is a constant power of 2.
The buffer size affects how often the `onaudioprocess` listener is called and
how many sample-frames the listener needs to process for each event.
The specification recommends a lower buffer size, since this tends to a lower
latency. Whereas a higher buffer size is taken to avoid [audio glitches][WebAudioAPI_audio_glitches],
that are caused by interruptions of the continuous stream.
The second parameter and third parameters are the number of input channels
and the number of output channels.

![Samples for the 440 Hz sound](/svg/morse-audio-data.svg "Samples for the 440 Hz sound")

The function to generate the data for the 440 Hz signal can be formulated using
the [function of time][Function_of_time] $$y(t) = A\sin(2\pi f t + \varphi)$$,
where $$A$$ is the amplitude, $$f$$ being the frequency, $$t$$ the time,
and $$\varphi$$ the phase.
The time $$t$$ can be computed using the sample rate. The sample rate is
the amount of samples or sample-frames per second. For instance if the
sample rate is 44100, then you can say that there are 44100 samples within one
second. Let the unit of the time be 1 second, $$A=1$$, $$f=440$$ and
$$\varphi = 0$$. It follows $$y(x) = \sin(2\pi fx/44100)$$, where $$x/44100 = t$$.
The stream of the sound data to the speakers can be started by
connecting the processor audio node to the destination of the audio context.
Similarly the stream can be stopped by disconnecting the processor node
from the destination of the audio context.

```javascript
var sound = (function (ctx) {
  var source = ctx.createBufferSource();
  var processor = ctx.createScriptProcessor(1024,1,1);
  var x = 0;
  var f = 440;
  source.connect(processor);
  processor.onaudioprocess = function (e) {
    var data = e.outputBuffer.getChannelData(0);
    for (var i = 0; i < data.length; i++) {
      data[i] = Math.sin(2 * Math.PI * f * (x++) / ctx.sampleRate);
    }
  };
  return {
    'play': function () { processor.connect(ctx.destination); },
    'stop': function () { processor.disconnect(); },
  };
})(new AudioContext());
```

## Distinguishing short and long signals

Two event handlers are registered, i.e. an onkeypress listener and an onkeyup
listener. Firstly the keypress event is fired before keyup.
Both listeners keep the time when the recent keypress or keyup event occured.
Then in case of a keyup event, the time difference between the recent keypress
and keyup event can be computed.
If the keypress event occurs the sound is played, and if the keyup event
occurs then the sound is stopped.

There are two thresholds. The first threshold is used to distinguish between
long and short signals. If the time difference reaches a certain
threshold, then the user interaction will be interpreted as a long signal,
otherwise the user interaction results in a short signal.
The second threshold is used to decide when to emit a sequence of signals.
The sequence of signals is the typed morse code and it can be mapped to a letter.

```javascript
// initialization
(function (morse, threshold, emitThreshold,
           resultElem, keyrElem, keypElem, sensor) {
  var keyd;
  var keyu;
  var keydownOccured = false;
  var keyupOccured = true;
  var timeout;

  document.body.onkeypress = function (e) {
    clearTimeout(timeout);
    keydownOccured = true;
    sound.play();
    // animate telegraph...
    if (keyupOccured) {
      keyd = new Date();
      if ((!!keyd && !!keyu) && ((keyd - keyu) > emitThreshold)) {
        morse.pop();
      }
      keyupOccured = false;
    }
  };
  document.body.onkeyup = function (e) {
    sound.stop();
    keyupOccured = true;
    // animate telegraph...
    keyu = new Date();
    // only accepting 'm'
    if (e.which === 77) {
      if (((keyu - keyd) < threshold)) {
        morse.push('•');
      } else {
        morse.push('–');
      }
    }
    // handling of backspace and space...
    keydownOccured = false;
    timeout = setTimeout(function () {
      if (!keydownOccured) { morse.pop(); }
    }, emitThreshold);
  };
})(morse, 80, 200
   document.getElementById('result'),
   document.getElementById('keyReleased'),
   document.getElementById('keyPressed'),
   document.getElementById('sensor'));
```

A small stack like JavaScript object is used to store the typed signals.
The `push` function is used to push another short or long signal on the
current stack of signals.
If the emit threshold is reached, the current sequence of signals corresponding
to the stack of signals is retrieved by calling the `pop` function which
takes all signals from the stack, decodes the signal sequence and writes it
to the UI.

```javascript
var morse = (function (resultElem) {
  var current = '';
  var codes = {
    '•–':   'A', '–•••': 'B', //...
  };
  return {
    push: function (c) { current += c; console.log(current); },
    pop: function () {
      if (current !== '') {
        if (!!codes[current]) {
          resultElem.innerHTML += codes[current];
        }
        current = '';
      }
    }
  };
})(document.getElementById('result'));
```

[Morse_code]: http://en.wikipedia.org/wiki/Morse_code
[WebAudioAPI_architecture]: http://webaudio.github.io/web-audio-api/#ModularRouting
[impl]: https://github.com/maltindal/morse-interface
[A440_pitch_standard]: https://en.wikipedia.org/wiki/A440_(pitch_standard)
[WebAudioAPI_createScriptProcessor]: http://webaudio.github.io/web-audio-api/#widl-BaseAudioContext-createScriptProcessor-ScriptProcessorNode-unsigned-long-bufferSize-unsigned-long-numberOfInputChannels-unsigned-long-numberOfOutputChannels
[WebAudioAPI_audio_glitches]: http://webaudio.github.io/web-audio-api/#audio-glitching
[Function_of_time]: https://en.wikipedia.org/wiki/Sine_wave
